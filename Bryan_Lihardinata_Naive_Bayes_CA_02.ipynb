{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bryan Lihardinata Naive Bayes CA-02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1yeZpTeFX3WUvACnDHERJLeugL8yZPMZN",
      "authorship_tag": "ABX9TyNrqIx9ZxtMWee8xAhEr1fi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blihardinata/CA02_Naive-Bayes/blob/main/Bryan_Lihardinata_Naive_Bayes_CA_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr3WNvSmh87R"
      },
      "source": [
        "# Spam Email Detection Using Naive Bayes Theorem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1_adqNJZ8u0"
      },
      "source": [
        "Bryan Lihardinata CA-02\n",
        "Link to github: https://github.com/blihardinata/CA02_Naive-Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0xyXKo-h9bC"
      },
      "source": [
        "#Import all necessary packages\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa_rm8Ruin_k"
      },
      "source": [
        "Step 1: Clean and prepare the data for a model then create a function to build dictionary of most common 3000 words from all the email content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOEv7raKjjQA"
      },
      "source": [
        "#Create a function to add all words and characters to the dictionary. \n",
        "def build_Dictionary(drive):\n",
        "  list_of_words = []\n",
        "  emails = [os.path.join(drive,file) for file in os.listdir(drive)]\n",
        "  for content in emails:\n",
        "    with open(content) as c:\n",
        "      for line in c:\n",
        "        words = line.split()\n",
        "        #adding and appending each word to the list of words\n",
        "        list_of_words += words\n",
        "#make a counter for each word in the dictionary. \n",
        "  dictionary = Counter(list_of_words)\n",
        "  junk = list(dictionary)\n",
        "  #print(junk)\n",
        "\n",
        "  for character in junk:\n",
        "#\tRemove non-alpha-numeric like #, : % etc\n",
        "    if character.isalpha() == False:\n",
        "      del dictionary[character]\n",
        "# Remove one alphabet word like \"a\", \"I\", \"U\" \n",
        "    elif len(character) == 1:\n",
        "      del dictionary[character]\n",
        "#Once all non-alpha-numeric and one alphabet are removed, save the 3000 most common words under a list called dictionary\n",
        "  dictionary = dictionary.most_common(3000) \n",
        "  #print(dictionary)\n",
        "  return dictionary\n",
        "\n",
        "#The newly build dictionary has 3000 most common words with all the stop words and non-alpha numeric characters removed. "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehF29I1TlwlU",
        "outputId": "f58bdffa-01e2-443e-8bd5-6ead059fd785"
      },
      "source": [
        "#Mount your drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDbgP4lk0kj8"
      },
      "source": [
        "Step 2: Create a function to extract features then build a feature matrix by analyzing the file path as well as the name of each email and separating spam Emails from non-spam Emails. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c02FEs6J0zXg"
      },
      "source": [
        "def extract_features(email_directory):\n",
        "  files = [os.path.join(email_directory,file) for file in os.listdir(email_directory)]\n",
        "  #len(files) is the count of how many files in a given folder\n",
        "  features_matrix = np.zeros((len(files),3000))\n",
        "  train_labels = np.zeros(len(files))\n",
        "  #print(train_labels) -> creating a list of 0\n",
        "  count = 1;\n",
        "  fileID = 0;\n",
        "  for doc in files:\n",
        "    with open(doc) as content:\n",
        "      for index, sentence in enumerate(content):\n",
        "        if index ==2:\n",
        "          words = sentence.split()\n",
        "          #print(words)\n",
        "          #splitting all words into a list of words in the given files\n",
        "          for word in words:\n",
        "            wordID = 0\n",
        "            #print(wordID)\n",
        "            #if the first value from \"variable\" is the same as the value in “word”, then the wordID count as \"index\"\n",
        "            for index, variable in enumerate(dictionary):\n",
        "              if variable[0] == word:\n",
        "                wordID = index\n",
        "                #save the fileID and wordID as a matrix as well as the count for each word\n",
        "                features_matrix[fileID,wordID] = words.count(word)\n",
        "                #print(features_matrix)\n",
        "      train_labels[fileID] = 0;\n",
        "      filepath = doc.split('/')\n",
        "      #print(filepath)\n",
        "      #save the last word in filepath as filename\n",
        "      filename = filepath[len(filepath)-1]\n",
        "      #print(filename)\n",
        "      if filename.startswith(\"spmsg\"):\n",
        "        train_labels[fileID] = 1;\n",
        "        count = count + 1\n",
        "      fileID = fileID + 1\n",
        "  return features_matrix, train_labels              "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CTxJ_HmGNOX"
      },
      "source": [
        "Step 3: Testing, training and predicting the accuracy rate of the model (or functions) performance \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7etUlptCGH8X"
      },
      "source": [
        "#Train and Test label directory\n",
        "TRAIN_DIR = '/content/drive/MyDrive/MSBA_Colab_2020/ML Algorithms/CA02/train-mails'\n",
        "TEST_DIR = '/content/drive/MyDrive/MSBA_Colab_2020/ML Algorithms/CA02/test-mails'"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_o2h_4Y_GMTu",
        "outputId": "68b4ffe6-9241-429b-8aa2-c0328cb2da8b"
      },
      "source": [
        "#Run the first four line to test whether our function is working or not\n",
        "dictionary = build_Dictionary(TRAIN_DIR)\n",
        "\n",
        "print (\"reading and processing emails from TRAIN and TEST folders\")\n",
        "#matrix of 702,3000 words\n",
        "features_matrix, labels = extract_features(TRAIN_DIR)\n",
        "#matrix of 260,3000 words\n",
        "test_features_matrix, test_labels = extract_features(TEST_DIR)\n",
        "\n",
        "model = GaussianNB()\n",
        "#building a table of train and test/validation model\n",
        "print (\"Training Model using Gaussian Naive Bayes algorithm .....\")\n",
        "model.fit(features_matrix, labels)\n",
        "print (\"Training completed\")\n",
        "print (\"testing trained model to predict Test Data labels\")\n",
        "predicted_labels = model.predict(test_features_matrix)\n",
        "print (\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "#accuracy score = similar to that of confusion matrix\n",
        "print (accuracy_score(test_labels, predicted_labels))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading and processing emails from TRAIN and TEST folders\n",
            "Training Model using Gaussian Naive Bayes algorithm .....\n",
            "Training completed\n",
            "testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9615384615384616\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}